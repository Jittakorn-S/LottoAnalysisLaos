use actix_files::Files;
use actix_web::{web, App, HttpResponse, HttpServer, Responder};
use lazy_static::lazy_static;
use scraper::{Html, Selector};
use serde::{Deserialize, Serialize};
use statrs::statistics::{Data, Distribution, Median, Min, Max};
use statrs::distribution::Normal;
use std::collections::HashMap;
use std::sync::Mutex;
use tokio::time::{sleep, Duration};

// --- Data Structures for Laos Lottery ---

#[derive(Serialize, Clone, Debug)]
struct LaosLottoResult {
    #[serde(rename = "Draw Date")]
    draw_date: String,
    #[serde(rename = "First Prize (3 Digits)")]
    first_prize: String,
    #[serde(rename = "Second Prize (2 Digits)")]
    second_prize: String,
}

#[derive(Serialize, Clone)]
struct TaskStatus {
    is_running: bool,
    lotto_type: Option<String>,
    progress: Vec<String>,
    results: Vec<LaosLottoResult>,
}

impl TaskStatus {
    fn new() -> Self {
        TaskStatus {
            is_running: false,
            lotto_type: None,
            progress: Vec::new(),
            results: Vec::new(),
        }
    }
}

lazy_static! {
    static ref TASK_STATUS: Mutex<TaskStatus> = Mutex::new(TaskStatus::new());
}

// --- Web Scraper for Laos Lottery ---

async fn scrape_laos_lotto_page(
    client: &reqwest::Client,
    url: &str,
) -> Result<(Vec<LaosLottoResult>, Option<String>), String> {
    let resp = client.get(url).send().await.map_err(|e| e.to_string())?;
    if !resp.status().is_success() {
        return Err(format!("Request failed with status: {}", resp.status()));
    }
    let body = resp.text().await.map_err(|e| e.to_string())?;
    let document = Html::parse_document(&body);

    let row_selector = Selector::parse(".mantine-Grid-root").unwrap();
    let col_selector = Selector::parse(".mantine-Grid-col").unwrap();
    let next_page_selector = Selector::parse("a[title='‡∏ú‡∏•‡∏´‡∏ß‡∏¢‡∏•‡∏≤‡∏ß‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á ‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≠‡πÑ‡∏õ']").unwrap();

    let mut page_results = Vec::new();
    for row in document.select(&row_selector) {
        let cols: Vec<_> = row.select(&col_selector).collect();
        if cols.len() >= 3 {
            let full_date_text = cols[0].text().collect::<String>();
            let date_text = full_date_text.split('|').nth(1).unwrap_or("").trim().to_string();

            let first_prize_raw = cols[1].text().collect::<String>().trim().to_string();
            let second_prize_raw = cols[2].text().collect::<String>().trim().to_string();

            if first_prize_raw == "‡∏á‡∏î‡∏≠‡∏≠‡∏Å‡∏ú‡∏•" || second_prize_raw == "‡∏á‡∏î‡∏≠‡∏≠‡∏Å‡∏ú‡∏•" || date_text.is_empty() {
                continue;
            }

            let first_prize: String = first_prize_raw.chars().filter(|c| c.is_digit(10)).take(3).collect();
            let second_prize: String = second_prize_raw.chars().filter(|c| c.is_digit(10)).take(2).collect();
            
            if !first_prize.is_empty() && !second_prize.is_empty() {
                 page_results.push(LaosLottoResult {
                    draw_date: date_text,
                    first_prize,
                    second_prize,
                });
            }
        }
    }
    
    // **IMPROVEMENT: Updated the URL construction as requested**
    let next_page_url = document
        .select(&next_page_selector)
        .next()
        .and_then(|a| a.value().attr("href"))
        .filter(|href| !href.contains("javascript"))
        .map(|s| format!("https://expalert.com/backward/{}", s));

    Ok((page_results, next_page_url))
}

async fn run_scraper() {
    let start_url = "https://expalert.com/backward/laosdevelops".to_string();
    let client = reqwest::Client::new();
    let mut all_results = Vec::new();
    let mut current_url = Some(start_url);

    while let Some(url) = current_url {
        { TASK_STATUS.lock().unwrap().progress.push(format!("üìÑ Scraping page: {}", url)); }
        match scrape_laos_lotto_page(&client, &url).await {
            Ok((mut page_results, next_url)) => { 
                all_results.append(&mut page_results); 
                current_url = next_url; 
            },
            Err(e) => { 
                TASK_STATUS.lock().unwrap().progress.push(format!("‚ö†Ô∏è Error scraping page {}: {}", url, e)); 
                current_url = None; 
            }
        }
        sleep(Duration::from_millis(1500)).await;
    }
    let mut status = TASK_STATUS.lock().unwrap();
    status.results = all_results;
    status.progress.push("‚úÖ Laos Lottery scraping complete.".to_string());
    status.is_running = false;
}

// --- Analysis Engine ---

#[derive(Deserialize)]
struct AnalyzeRequest {
    numbers: Vec<String>,
}

#[derive(Serialize)]
struct AnalysisResponse {
    statistical_summary: HashMap<String, String>,
    pattern_analysis: HashMap<String, serde_json::Value>,
    prediction_output: HashMap<String, serde_json::Value>,
    detailed_explanation: HashMap<String, String>,
}

fn run_comprehensive_analysis(numbers_str: &[String]) -> Result<AnalysisResponse, String> {
    if numbers_str.len() < 10 { return Err(format!("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ AI ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ä‡∏∏‡∏î‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 10 ‡∏ä‡∏∏‡∏î ‡πÅ‡∏ï‡πà‡∏û‡∏ö‡πÄ‡∏û‡∏µ‡∏¢‡∏á {} ‡∏ä‡∏∏‡∏î", numbers_str.len())); }
    let numbers_f64: Vec<f64> = numbers_str.iter().filter_map(|s| s.parse::<f64>().ok()).collect();
    if numbers_f64.len() < 5 { return Err("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÑ‡∏î‡πâ".to_string()); }

    let data = Data::new(numbers_f64.clone());
    let mean = data.mean().unwrap_or(0.0);
    let median = data.median();
    let std_dev = data.std_dev().unwrap_or(0.0);
    let variance = data.variance().unwrap_or(0.0);
    let min = data.min();
    let max = data.max();
    let skewness = Normal::new(mean, std_dev).unwrap().skewness().unwrap_or(0.0);
    
    let mut counts = HashMap::new();
    for s in numbers_str { *counts.entry(s.clone()).or_insert(0) += 1; }
    
    let mode = counts.iter().max_by_key(|&(_, count)| count).map(|(val, _)| val.clone()).unwrap_or_else(|| "N/A".to_string());

    let statistical_summary = HashMap::from([
        ("Dataset Size".to_string(), numbers_str.len().to_string()),
        ("Mean".to_string(), format!("{:.2}", mean)),
        ("Median".to_string(), format!("{:.2}", median)),
        ("Mode (‡∏ê‡∏≤‡∏ô‡∏ô‡∏¥‡∏¢‡∏°)".to_string(), mode.clone()),
        ("Std. Dev.".to_string(), format!("{:.2}", std_dev)),
        ("Variance".to_string(), format!("{:.2}", variance)),
        ("Range".to_string(), format!("{:.2} - {:.2}", min, max)),
        ("Distribution Skewness".to_string(), format!("{:.4}", skewness)),
    ]);

    let most_frequent: Vec<String> = counts.iter().take(10).map(|(k, v)| format!("{} ({} times)", k, v)).collect();
    
    let mut digit_pos_freq: HashMap<usize, HashMap<char, usize>> = HashMap::new();
    for num_str in numbers_str {
        for (i, c) in num_str.chars().enumerate() {
            *digit_pos_freq.entry(i).or_default().entry(c).or_default() += 1;
        }
    }
    let digit_analysis_str: Vec<String> = digit_pos_freq.iter()
        .map(|(pos, freqs)| {
            let top_digit = freqs.iter().max_by_key(|&(_, count)| count).map(|(d, c)| format!("'{}' ({} times)", d, c)).unwrap_or_default();
            format!("Position {}: Most frequent is {}", pos + 1, top_digit)
        }).collect();

    let pattern_analysis = HashMap::from([
        ("Most Frequent Numbers".to_string(), serde_json::json!(most_frequent)),
        ("Digit & Position Analysis".to_string(), serde_json::json!(digit_analysis_str)),
    ]);
    
    let main_prediction = mode;
    let alternatives: Vec<String> = counts.iter().filter(|(k, _)| **k != main_prediction).take(4).map(|(k, _)| k.clone()).collect();
    let confidence = (60.0 + (numbers_str.len() as f64 / 100.0 * 20.0)).min(95.0);

    let prediction_output = HashMap::from([
        ("PREDICTION".to_string(), serde_json::json!(main_prediction.clone())),
        ("CONFIDENCE".to_string(), serde_json::json!(format!("{:.2}%", confidence))),
        ("METHOD".to_string(), serde_json::json!("Weighted Statistical & Frequency Model")),
        ("ALTERNATIVE_PREDICTIONS".to_string(), serde_json::json!(alternatives)),
    ]);

    let explanation = HashMap::from([
        ("Methodology".to_string(), "‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ú‡∏™‡∏°‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà (Frequency Analysis) ‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (Statistical Significance) ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (Mode) ‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å".to_string()),
        ("Statistical Evidence".to_string(), format!("‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç '{}' ‡πÄ‡∏õ‡πá‡∏ô‡∏ê‡∏≤‡∏ô‡∏ô‡∏¥‡∏¢‡∏° (Mode) ‡∏ã‡∏∂‡πà‡∏á‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏ö‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏ö‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà {:.2} ‡∏ã‡∏∂‡πà‡∏á‡∏ö‡πà‡∏á‡∏ä‡∏µ‡πâ‡∏ñ‡∏∂‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", main_prediction, std_dev)),
        ("Prediction Logic".to_string(), "‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏Å‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏Ñ‡πà‡∏≤‡∏ê‡∏≤‡∏ô‡∏ô‡∏¥‡∏¢‡∏° (Mode) ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏ö‡πà‡∏á‡∏ä‡∏µ‡πâ‡∏ó‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏ó‡∏µ‡πà‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î‡∏ô‡∏µ‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏≠‡∏≠‡∏Å‡∏ã‡πâ‡∏≥ ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏•‡∏á‡∏°‡∏≤".to_string()),
        ("Uncertainty Analysis".to_string(), "‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏à‡∏≤‡∏Å‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡πà‡∏ô‡∏ä‡∏±‡∏î‡∏Ç‡∏≠‡∏á‡∏ê‡∏≤‡∏ô‡∏ô‡∏¥‡∏¢‡∏° ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô".to_string()),
    ]);

    Ok(AnalysisResponse { statistical_summary, pattern_analysis, prediction_output, detailed_explanation: explanation })
}

// --- API Endpoints ---

#[derive(Deserialize)]
struct StartScrapeRequest {
    lotto_type: String,
}

async fn start_scrape(req: web::Json<StartScrapeRequest>) -> impl Responder {
    let mut status = TASK_STATUS.lock().unwrap();
    if status.is_running { return HttpResponse::Conflict().json(serde_json::json!({"error": "A scraper is already running."})); }
    if req.lotto_type != "laos" { return HttpResponse::BadRequest().json(serde_json::json!({"error": "Invalid lottery type. Only 'laos' is supported."})); }
    
    status.is_running = true;
    status.lotto_type = Some(req.lotto_type.clone());
    status.progress = vec!["üöÄ Starting scraper for Laos Lottery...".to_string()];
    status.results.clear();
    tokio::spawn(run_scraper());
    HttpResponse::Accepted().json(serde_json::json!({"message": "Scraping process started!"}))
}

async fn get_status() -> impl Responder { HttpResponse::Ok().json(&*TASK_STATUS.lock().unwrap()) }

async fn analyze_handler(req: web::Json<AnalyzeRequest>) -> impl Responder {
    match run_comprehensive_analysis(&req.numbers) {
        Ok(response) => HttpResponse::Ok().json(response),
        Err(e) => HttpResponse::BadRequest().json(serde_json::json!({ "error": e })),
    }
}

async fn index() -> impl Responder {
    match std::fs::read_to_string("templates/index.html") {
        Ok(content) => HttpResponse::Ok().content_type("text/html; charset=utf-8").body(content),
        Err(_) => HttpResponse::InternalServerError().body("Could not read index.html"),
    }
}

#[actix_web::main]
async fn main() -> std::io::Result<()> {
    let port_str = std::env::var("PORT").unwrap_or_else(|_| "8080".to_string());
    let port = port_str.parse::<u16>().expect("PORT must be a valid number");
    if !std::path::Path::new("templates/index.html").exists() { eprintln!("‚ùå Error: templates/index.html not found."); }
    println!("üåç Server starting at http://0.0.0.0:{}", port);

    HttpServer::new(|| {
        App::new()
            .route("/", web::get().to(index))
            .route("/start-scrape", web::post().to(start_scrape))
            .route("/status", web::get().to(get_status))
            .route("/analyze", web::post().to(analyze_handler))
            .service(Files::new("/static", "static").show_files_listing())
    })
    .bind(("0.0.0.0", port))?
    .run()
    .await
}